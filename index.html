<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Useful References</title>
    <link rel="stylesheet" href="https://errickson.net/style.css">
    <link rel="stylesheet" href="style.css">
    <script src="toc.js"></script>
  </head>
  <body class="index">
    <h1 class="centered">Useful References</h1>
    <br />
    <div class="centered">
      <input type="text" id="filterInput" placeholder="Type to filter..." />
      <button id="resetButton">Reset</button>
    </div>
    <ul class="citation_list">

      <li class="citation_entry">
        <div class="bib">
          <p class="title"><a href="https://doi.org/10.1038/s41562-017-0189-z">Redefine
            statistical significance.</a></p>
          <p class="title_details">Benjamin et al., 2018</p>
        </div>

        <p>
          This paper suggests using &alpha; =.005 instead of &alpha; = .05 for
          significance in exploratory analysis. They also suggest calling .005
          &lt; <emph>p</emph> &lt; .05 as "suggestive".
        </p>

        <p>
          The paper uses a Bayesian approach, arguing that &alpha; = .005
          corresponds to a Bayes Factor of 14 to 26, which translates (according
          to <a href="https://doi.org/10.1080/01621459.1995.10476572">Kass &
          Rafferty (1993)</a>) to "substantial" to "strong" evidence.
          </p>
      </li>

      <li class="citation_entry">
        <div class="bib">
          <p class="title"><a href="https://doi.org/10.1136/bmj.311.7005.619">Probability
          of adverse events that have not yet occurred: a statistical reminder</a></p>
          <p class="title_details">Eypasch et al., 1995</p>
        </div>

        <p>
          A very quick discussion about the
          <a href="https://en.wikipedia.org/wiki/Rule_of_three_(statistics)">rule
            of three</a>, establishing estimation of a confidence interval in
          scenarios where the prevalance of a rare event is 0.
        </p>

        <div class="bib">
          <p class="title"><a href="https://doi.org/10.1080/00031305.1997.10473947">A
            Look at the Rule of Three</a></p>
          <p class="title_details">Jovanovic & Levy, 19957</p>
        </div>

        <p>
          A more in-depth explanation, including derivations of the rule and a
          discussion of a correction for small sample sizes (n &lt; 100).
        </p>
      </li>

      <li class="citation_entry">
        <div class="bib">
          <p class="title"><a href="https://doi.org/10.6339/JDS.201101_09%281%29.0002">Asymptotic
            Equivalence between Cross-Validations and Akaike Information
            Criteria in Mixed-Effects Models</a></p>
          <p class="title_details">Fang, 2011</p>
        </div>

        <p>
          Shows that leave-one-out cross validation (LOOCV) is asymptotically
          equivalent to AIC when used with mixed effects regression models.
        </p>
      </li>

      <li class="citation_entry">
        <div class="bib">
          <p class="title"><a href="https://doi.org/10.1093/biostatistics/1.1.27">Should We Take Measurements at an Intermediate Design Point?</a></p>
          <p class="title_details">Gelman, 2000</p>
        </div>

        <p>
          When collecting longitudinal data, is it worth collecting intermediate
          data points, versus collecting alarger sample size with only two time
          points. The conclusion is that it is not worth collecting the
          intermediate data point unless the sample size is very large and
          non-linearity is strongly assumed.
          </p>
      </li>

      <li class="citation_entry">
        <div class="bib">
          <p class="title"><a href="https://doi.org/10.1198/000313001300339897">The
            Abuse of Power: The Pervasive Fallacy of Power Calculations for Data
            Analysis</a></p>
          <p class="title_details">Hoenig & Heisey, 2012</p>
        </div>

        <p>
          A discussion of the inappropriate use of post-hoc power analysis. They
          argue to never to do post-hoc power analyses, and suggest reporting
          confidence intervals instead of the power analysis as a method to
          describe the uncertainty and strength of evidence.
        </p>
      </li>

      <li class="citation_entry">
        <div class="bib">
          <p class="title"><a href="https://doi.org/10.3200/JECE.36.1.77-92">Oh
          No! I Got the Wrong Sign! What Should I Do?</a></p>
          <p class="title_details">Kennedy, 2010</p>
        </div>

        <p>
          Lists around 40 different reasons why a "wrong sign" can be obtained,
          and offers very brief advice on addressing it. It is published in the
          Econ literature so is specific there, but a lot of entries on the list
          are more generic.
        </p>

        <p>
          Categories of examples include:
        </p>
          <ul style="list-style-type: disc">
            <li>Bad theory (e.g. the assumed sign is wrong, the observed sign is correct)</li>
            <li>Bad interpretation (e.g. ignoring interaction terms)</li>
            <li>Bad data</li>
            <li>Statistical issues (e.g. outliers, measurement error)</li>
          </ul>

      </li>

      <li class="citation_entry">
        <p>LeeEtAll_2021_Framework_for_the_treatment_and_reporting_of_missing_data_in_observational_studies.txt</p>

        Excellent overview of types of missing data, whether MI is needed, and whether sensitivity analysis is needed
      </li>
      <li class="citation_entry">
        <p>MacCallumEtAll_2015_Power_Analysis_and_Determination_of_Sample_Size_for_Covariance_Structure_Modeling.txt</p>

        Sample size estimates for SEM models.
      </li>
      <li class="citation_entry">
        <p>McNeish_and_Kelley_2019_Fixed_Effects_Models_Versus_Mixed_Effects_Models_for_Clustered_Data.txt</p>

        Provides a discussion of what to use mixed models and when to use fixed effects models.

        The big argument for fixed effects is omitted variable bias and
        endogeneity at level 2 does not impact level 1 estimates. Also a lesser argument that
        <30 clusters is bad for mixed model.

            They also suggest a "within-between mixed model", e.g. group-mean centering predictors and also including group means as predictors. E.g. something like

            y_ij = ... + beta*(x_ij - xbar_j) + alpha*xbar_j

            This splits within and between completely.
      </li>
      <li class="citation_entry">
        <p>Rhemtulla_2012_When_Can_Categorical_Variables_Be_Treated_As_Continuous.txt</p>

        A simulation showing when categorical variables can be treated as
        continuous without bias. Specifically for CFA. Conclusion:

        "Our findings confirm that when observed variables have fewer than
        five categories, robust categorical methodology is best. With five to
        seven categories, both methods yield acceptable performance."
      </li>
      <li class="citation_entry">
        <p>Schenker_and_Gentleman_2001_On_Judging_the_Significance_of_Differences_by_Exam.txt</p>

        tldr: If two CI's don't overlap, you can reject the null that point
        estimates are the same. If they don't overlap, you need to test the
        hypothesis that x1 - x2 = 0.


        If viewing two confidence intervals for two point estimates, it is a
        common practice to see whether the confidence intervals overlap;
        failure to overlap provides evidence that the two points differ
        statistically.

        Call this method the "overlap method" and call hypothesis testing (x1
        - x2) = 0 as the "standard method".

        If the overlap method rejects, then the standard method rejects.
        (If the two CI's don't overlap, a hypothesis test will find a
        significant difference.)

        If the overlap method fails to reject, then the standard method may
        reject or not reject.
        (If the two CI's reject, we don't know what a hypothesis test will
        do.)

        Two CI's can overlap and a hypothesis test still find them
        signficant. This is more likely when the standard error of each point
        is simialr (e.g. the amount of overlap for which the hypothesis test
        fails to reject is largest if the CI's are of equal width).


        A different approach - reject if neither CI contains the other point
        estimate - has issues the other way. No overlap here does not imply
        significance in the hypothesis test.


        Correlation between the point estimates makes the problem
        worse. (Above was assuming independence.)
      </li>
      <li class="citation_entry">
        <p>Schielzeth_2020_Robustness_of_linea_mixed-effects_models_to_violation_of_distributional_assumptions.txt</p>

        Model estimates were usually robust to violations of assumptions, with the
        exception of slight upward biases in estimates of random effect variance if the
        generating distribution was bimodal but was modelled by Gaussian error
        distributions. Further, estimates for (random effect) components that violated
        distributional assumptions became less precise but remained unbiased. However,
        this particular problem did not affect other parameters of the model.

        E.g. normality is not important!
      </li>
      <li class="citation_entry">
        <p>Schoemann_EtAl_2017_Determining_Power_and_Sample_size_for_Simple_and_Complex_Mediation_Models.txt</p>

        Methodology and link to a tool to perform MCMC for power analysis of mediation model.

        Link: https://schoemanna.shinyapps.io/mc_power_med/
      </li>
      <li class="citation_entry">
        <p>Stensrud_and_Hernan_2020_Why_test_for_proportional_hazards.txt</p>

        Claims:
        - Hazards are never proportional; failure to reject PH-tests implies lack of power.
        - Use bootstrap to properly estimate SE's (other sources suggested robust SE).
        - "a hazard ratio from a Cox model needs to be interpreted as a weighted average of the true hazard ratios over the entire follow-up period."
      </li>
      <li class="citation_entry">
        <p>Waite_and_Campbell_2006_Controlling_the_false_discovery_rate_and_increasin.txt</p>

        Familywise error rate (FWER) - Rank p-values, reject first at .05/m,
        reject second at .05/(m-1), third at .05/(m-2), etc.

        Argument against FWER is that it increases Type II error as
        significance depends on the number of other tests done. Also suffer
        from reduced power.

        Instead, control for false discovery rate (FDR) using
        Benjamini-Hochberg (BH) procedure.

        Bonferroni - 95% certainty there are NO false discoveries.

        FDR/BH - At most 5% of signifant findings are false discoveries. (If
        further citation needed, Benjamini & Hochberg, 1995.) If a large
        number of tests, P(At least one false discovery) increases, but only
        5% of those discoveries will be false.

        Methodologies:

        Order all p-values from low to high. Say there are m total p-values.

        Bonferroni FWER: Reject lowest p-value at .05/m. Reject second lowest
        p-value at .05/(m-1). Etc. Reject highest p-value at .05/(m-[m-1]) =
        .05.

        BH FDR: Reject lower p-value at 1*.05/m. Reject second lowest p-value
        at 2*.05/m. Reject third lowest p-value at 3*.05/m. Etc. Reject
        highest p-value at m*.05/m = .05.
      </li>
      <li class="citation_entry">
        <p>von_Hippel_2018_How_Many_Imputations_Do_You_Need.txt</p>

        Rebuttal of the typical "m = 5 imputations" recommendation.

        The m = 5 (or as this paper puts it, 2-10) is sufficient for
        efficienct estimation of point estimation, not for standard error of
        point estimate. Suggests m = 200 for sample sizes where this
        reasonable.

        Introduces a two-stage procedure.

        1) Perform a "pilot" MI on 5-20 imputations. Using the fraction of
        missing information [1 - var(point estimate | complete
        data)/var(point estimate | M imputations)], calculate how many
        total imputations are needed.  Specifically, use a "conservative"
        fraction of missing information, based on CI (so larger M in pilot
        will lead to more precision in M for follow-up.)

        2) Use the new M to run the imputation.

        Fraction of missing information is NOT % of missing data, however, you
        can get rough estimates. The rule of thumb I get out of this is that

        For little missing information, use m = 50.
        For moderate missing information, use m = 200.
        For large missing information, use m = 400.


        The quadratic rule is 200*(fraction of missing information)^2.

        Implemented in Stata via `how_many_imputations`:

        ```
        webuse mheart1s0, clear
        mi impute regress bmi attack smokes age female hsgrad, replace add(10)
        mi estimate: logit attack bmi smokes age female hsgrad
        how_many_imputations, cv_se(.01)
        mi impute regress bmi attack smokes age female hsgrad, add(`r(add_M)')
        mi estimate: logit attack bmi smokes age female hsgrad
        ```

      </li>    </ul>

    <p style="text-align: center">
      <a href="https://errickson.net">Home</a>
    </p>

    <script src="index_filter.js"></script>
  </body>
</html>
