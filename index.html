<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Useful References</title>
    <link rel="stylesheet" href="https://errickson.net/style.css">
    <link rel="stylesheet" href="style.css">
    <script src="toc.js"></script>
  </head>
  <body class="index">
    <h1 class="centered">Useful References</h1>
    <br >
    <div class="centered">
      <input type="text" id="filterInput" placeholder="Type to filter..." >
      <button id="resetButton">Reset</button>
    </div>
    <ul class="citation_list">

      <li class="citation_entry">
        <div class="bib">
          <p class="title"><a href="https://doi.org/10.1038/s41562-017-0189-z">Redefine
            statistical significance.</a></p>
          <p class="title_details">Benjamin et al., 2018</p>
        </div>

        <p>
          This paper suggests using &alpha; =.005 instead of &alpha; = .05 for
          significance in exploratory analysis. They also suggest calling .005
          &lt; <i>p</i> &lt; .05 as "suggestive".
        </p>

        <p>
          The paper uses a Bayesian approach, arguing that &alpha; = .005
          corresponds to a Bayes Factor of 14 to 26, which translates (according
          to <a href="https://doi.org/10.1080/01621459.1995.10476572">Kass &
          Rafferty (1993)</a>) to "substantial" to "strong" evidence.
          </p>
      </li>

      <li class="citation_entry">
        <div class="bib">
          <p class="title"><a href="https://doi.org/10.1136/bmj.311.7005.619">Probability
          of Adverse Events That Have Not Yet Occurred: A Statistical Reminder</a></p>
          <p class="title_details">Eypasch et al., 1995</p>
        </div>

        <p>
          A very quick discussion about the
          <a href="https://en.wikipedia.org/wiki/Rule_of_three_(statistics)">rule
            of three</a>, establishing estimation of a confidence interval in
          scenarios where the prevalance of a rare event is 0.
        </p>

        <div class="bib">
          <p class="title"><a href="https://doi.org/10.1080/00031305.1997.10473947">A
            Look at the Rule of Three</a></p>
          <p class="title_details">Jovanovic & Levy, 19957</p>
        </div>

        <p>
          A more in-depth explanation, including derivations of the rule and a
          discussion of a correction for small sample sizes (n &lt; 100).
        </p>
      </li>

      <li class="citation_entry">
        <div class="bib">
          <p class="title"><a href="https://doi.org/10.6339/JDS.201101_09%281%29.0002">Asymptotic
            Equivalence between Cross-Validations and Akaike Information
            Criteria in Mixed-Effects Models</a></p>
          <p class="title_details">Fang, 2011</p>
        </div>

        <p>
          Shows that leave-one-out cross validation (LOOCV) is asymptotically
          equivalent to AIC when used with mixed effects regression models.
        </p>
      </li>

      <li class="citation_entry">
        <div class="bib">
          <p class="title"><a href="https://doi.org/10.1093/biostatistics/1.1.27">Should We Take Measurements at an Intermediate Design Point?</a></p>
          <p class="title_details">Gelman, 2000</p>
        </div>

        <p>
          When collecting longitudinal data, is it worth collecting intermediate
          data points, versus collecting alarger sample size with only two time
          points. The conclusion is that it is not worth collecting the
          intermediate data point unless the sample size is very large and
          non-linearity is strongly assumed.
          </p>
      </li>

      <li class="citation_entry">
        <div class="bib">
          <p class="title"><a href="https://doi.org/10.1198/000313001300339897">The
            Abuse of Power: The Pervasive Fallacy of Power Calculations for Data
            Analysis</a></p>
          <p class="title_details">Hoenig & Heisey, 2012</p>
        </div>

        <p>
          A discussion of the inappropriate use of post-hoc power analysis. They
          argue to never to do post-hoc power analyses, and suggest reporting
          confidence intervals instead of the power analysis as a method to
          describe the uncertainty and strength of evidence.
        </p>
      </li>

      <li class="citation_entry">
        <div class="bib">
          <p class="title"><a href="https://doi.org/10.3200/JECE.36.1.77-92">Oh
          No! I Got the Wrong Sign! What Should I Do?</a></p>
          <p class="title_details">Kennedy, 2010</p>
        </div>

        <p>
          Lists around 40 different reasons why a "wrong sign" can be obtained,
          and offers very brief advice on addressing it. It is published in the
          Econ literature so is specific there, but a lot of entries on the list
          are more generic.
        </p>

        <p>
          Categories of examples include:
        </p>
        <ul style="list-style-type: disc">
          <li>Bad theory (e.g. the assumed sign is wrong, the observed sign is
            correct)</li>
          <li>Bad interpretation (e.g. ignoring interaction terms)</li>
          <li>Bad data</li>
          <li>Statistical issues (e.g. outliers, measurement error)</li>
        </ul>

      </li>

      <li class="citation_entry">
        <div class="bib">
          <p class="title"><a href="https://doi.org/10.1016/j.jclinepi.2021.01.008">Framework
          for the Treatment and Reporting of Missing Data in Observational
          Studies: The Treatment and Reporting of Missing Data in Observational
          Studies Framework</a></p>
          <p class="title_details">Lee et al., 2021</p>
        </div>

        <p>
          An excellent overview of the types of missing data (missing at random,
          not random, completely at random) and whether multiple imputation (MI)
          is needed or whether complete-case analysis is sufficient.
        </p>

        <div class="bib">
          <p class="title"><a href="https://doi.org/10.1016/j.jclinepi.2019.02.016">The
          Proportion of Missing Data Should Not Be Used to Guide Decisions on
          Multiple Imputation</a></p>
          <p class="title_details">Madley-Dowd et al., 2019</p>
        </div>

        <p>
          Shows that in situations with large proportion of missing data (up to
          90%), there is still a benefit to carrying out MI. The paper also
          discusses the advice to simply do complete-case analysis if doing so
          will lead to under 5% of observations lost to missing data.
          </p>
      </li>

      <li class="citation_entry">
        <div class="bib">
          <p class="title"><a href="https://psycnet.apa.org/doi/10.1037/1082-989X.1.2.130">Power
          Analysis and Determination of Sample Size for Covariance Structure
          Modeling</a></p>
          <p class="title_details">MacCallum et al., 1996</p>
        </div>

        <p>
          Provides a way to estimate the sample size needed for various
          Structure Equation Models (SEM)/Path Analyses. The results are based
          upon the degrees of freedom of the model and the theorized effect
          size. Table 4 is particularly useful.
        </p>

        <p>
          (Wanted: A good citation explaining how to estimate DF in an SEM, with
          visual examples!)
          </p>
      </li>

      <li class="citation_entry">
        <div class="bib">
          <p class="title"><a href="https://psycnet.apa.org/doi/10.1037/met0000182">Fixed
          Effects Models Versus Mixed Effects Models for Clustered Data:
          Reviewing the Approaches, Disentangling the Differences, and Making
          Recommendations.</a></p>
          <p class="title_details"></p>
        </div>

        <p>
          A discussion of when to use fixed effects regression models, versus
          when to use mixed effects regression models (also called "random
          effects regression models" in Econometrics).
        </p>

        <p>
          Their primary argument for fixed effects is omitted variable bias and
          endogeneity at level 2 (group) does not impact level 1 (individual)
          estimates. A secondary argument is that mixed effects models require
          at least 30 groups. (Here's an <a href="https://stats.stackexchange.com/questions/37647/what-is-the-minimum-recommended-number-of-groups-for-a-random-effects-factor">online
          discussion arguing for lower thresholds</a> with several different
          citations).
        </p>

        <p>
          The paper also suggests another alternative, the "within-between mixed
          model", e.g. group-mean centering predictors and also including group
          means as predictors. This models splits within and between completely.
          For example, if you have some predictor <i>X</i>, include in the model
          both <i><span style="text-decoration: overline;">X</span><sub>i</sub></i>
          (the group-level mean) and <i>X<sub>ij</sub> -
          <span style="text-decoration: overline;">X</span><sub>i</sub></i> (the
          variable, group-mean centered).
      </li>

      <li class="citation_entry">
        <div class="bib">
          <p class="title"><a href="https://psycnet.apa.org/doi/10.1037/a0029315">When
          Can Categorical Variables Be Treated as Continuous? A Comparison of
          Robust Continuous and Categorical SEM Estimation Methods Under
          Suboptimal Conditions</a></p>
          <p class="title_details">Rhemtulla et al., 2012</p>
        </div>

        <p>
          A simulation showing when categorical variables can be treated as
          continuous without bias, specifically for confirmatory factor analysis
          (CFA). Their conclusion:
        </p>

        <blockquote>
          <div>
            Our findings confirm that when observed variables have fewer than
            five categories, robust categorical methodology is best. With five
            to seven categories, both methods yield acceptable performance.
          </div>
        </blockquote>
      </li>

      <li class="citation_entry">
        <div class="bib">
          <p class="title"><a href="https://doi.org/10.1198/000313001317097960">On
          Judging the Significance of Differences by Examining the Overlap
          Between Confidence Intervals</a></p>
          <p class="title_details">Schenker & Gentleman, 2001</p>
        </div>

        <p>
          Briefly:
        </p>
        <ul style="list-style-type: disc">
          <li>If two confidence intervals's do not overlap, you can reject
            the null that point estimates are the same.</li>
          <li>If they don't overlap, you cannot make any conclusion and need
            to do a full hypothesis test.</li>
        </ul>

        <p>
          Two confidence intervals overlapping and still be statistically
          significant via a hypothesis stest is more likely when the standard
          errors are similar.
        </p>

      </li>

      <li class="citation_entry">
        <div class="bib">
          <p class="title"><a href="https://doi.org/10.1111/2041-210X.13434">Robustness
          of Linear Mixed-Effects Models to Violations of Distributional
          Assumptions</a></p>
          <p class="title_details">Schielzeth et al., 2020</p>
        </div>

        <p>
          Model estimates were usually robust to violations of assumptions, with
          the exception of slight upward biases in estimates of random effect
          variance if the generating distribution was bimodal but was modelled
          by normal error distributions. Further, estimates for random effect
          components that violated distributional assumptions became less
          precise but remained unbiased. However, this particular problem did
          not affect other parameters of the model.
        </p>

        <p>
          In other words, normality is not that important!
        </p>
      </li>

      <li class="citation_entry">
        <div class="bib">
          <p class="title"><a href="https://doi.org/10.1177/1948550617715068">Determining
          Power and Sample Size for Simple and Complex Mediation Models</a></p>
          <p class="title_details">Schoemann et al., 2017</p>
        </div>

        <p>
          A method for performing Monte Carlo Markov Chain (MCMC) estimation of
          power analysis for mediation models. They also provide this tool:
          <a href="https://schoemanna.shinyapps.io/mc_power_med/">https://schoemanna.shinyapps.io/mc_power_med/</a>.
          It's a bit slow and as with any complex model power calculation,
          requires a lot of assumptions or knowledge of the data.
      </li>

      <li class="citation_entry">
        <div class="bib">
          <p class="title"><a href="https://doi.org/10.1001/jama.2020.1267">Why
          Test for Proportional Hazards?</a></p>
          <p class="title_details">Stensrud & Hernan, 2020</p>
        </div>

        <p>
          Three key claims:
        </p>
        <ul style="list-style-type: disc">
          <li>Hazards are never proportional; failure to reject proportional
            hazard tests implies a lack of power instead.</li>
          <li>Use bootstrap to properly estimate standard errors's (other
            sources suggested robust standard errors instead).</li>
          <li><blockquote>"a hazard ratio from a Cox model needs to be
            interpreted as a weighted average of the true hazard ratios over
            the entire follow-up period."</blockquote></li>
        </ul>
      </li>

      <li class="citation_entry">
        <div class="bib">
          <p class="title"><a href="https://doi.org/10.1177/0049124117747303">How Many Imputations Do You Need? A Two-stage Calculation Using a Quadratic Rule</a></p>
          <p class="title_details">Von Hippel, 2018</p>
        </div>
        <p>von_Hippel_2018_How_Many_Imputations_Do_You_Need.txt</p>

        <p>
          A rebuttal of the typical "5 imputations is enough" recommendation.
        </p>

        <p>
          The m = 5 (or as this paper puts it, 2-10) is sufficient for efficient
          estimation of point estimation, not for standard error of point
          estimate. This paper suggests 200 imputations for sample sizes where
          this reasonable.
        </p>

        <p>
          They also introduce a two-stage procedure to produce an estimate of
          the total number of imputations needed. It is implemented in Stata and
          SAS by <a href="https://missingdata.org">von Hippel</a>, and in R via
          the <a href="https://errickson.net/howManyImputations/"><strong>howManyImputations</strong></a> package.
      </li>
    </ul>

    <p style="text-align: center">
      <a href="https://errickson.net">Home</a>
    </p>

    <script src="index_filter.js"></script>
    <img src="https://errickson.goatcounter.com/count?p=citations" alt="Image tracker for goatcounter">
  </body>
</html>
